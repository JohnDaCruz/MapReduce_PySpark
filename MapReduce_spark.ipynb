{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Importando libs fundamentais\n",
        "import findspark\n",
        "import numpy as np\n",
        "\n",
        "#Iniciando findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "zpbkFi5TL3Vj"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "ouU1hScnHEXr"
      },
      "outputs": [],
      "source": [
        "#Criando um contexto Spark para desenvolvimento\n",
        "from pyspark import SparkContext\n",
        "spark_contexto = SparkContext()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Criandum array numpy\n",
        "vetor = np.array([10, 20, 30, 40, 50])\n",
        "\n",
        "#Transformando um conjunto de informações Numpy em RDD Spark\n",
        "paralelo = spark_contexto.parallelize(vetor)\n",
        "\n",
        "print(vetor,'--> ',paralelo)"
      ],
      "metadata": {
        "id": "bcqot0TlL4ie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b648fc-568e-4c8e-87be-73bb4253c406"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10 20 30 40 50] -->  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mapeando a variável 'pararelo' aplicando a função a cada elemento\n",
        "mapa = paralelo.map(lambda x : x**2+x)\n",
        "mapa.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18sfxj-uTtul",
        "outputId": "332152f8-05a9-42be-d966-5e12959c2db4"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[110, 420, 930, 1640, 2550]"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Criando um objeto RDD com parallelize\n",
        "texto = 'O Apache Spark é uma ferramenta Big Data que tem o objetivo de processar grandes conjuntos de dados de forma paralela e distribuída. Ela estende o modelo de programação MapReduce popularizado pelo Apache Hadoop, facilitando bastante o desenvolvimento de aplicações de processamento de grandes volumes de dados. Além do modelo de programação estendido, o Spark também apresenta uma performance muito superior ao Hadoop, chegando em alguns casos a apresentar uma performance quase 100x maior.'.split()\n",
        "\n",
        "paralelo = spark_contexto.parallelize(texto)\n",
        "print(type(paralelo))\n",
        "\n",
        "#Aplicando a função a cada elemento\n",
        "funcao_lambda = lambda x:(x,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf2VFS7gYCqn",
        "outputId": "94514921-0e3a-40ca-efa3-18022a4f72cb"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mapeando 'paralelo' e atribuindo um par 'chave:valor' com 'funcao_lambda'\n",
        "# reduceByKey() a transformação é usada para mesclar os valores de cada chave\n",
        "from operator import add\n",
        "mapa = paralelo.map(funcao_lambda).reduceByKey(add).collect()"
      ],
      "metadata": {
        "id": "E_yG4dZibHcD"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (w, c) in mapa:\n",
        "  print(\"{}: {}\".format(w, c))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvZxoLkcbIMM",
        "outputId": "c4f9bdeb-09be-487c-8e75-bb3aa2f79949"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O: 1\n",
            "Apache: 2\n",
            "Spark: 2\n",
            "uma: 3\n",
            "que: 1\n",
            "tem: 1\n",
            "objetivo: 1\n",
            "processar: 1\n",
            "dados: 1\n",
            "paralela: 1\n",
            "estende: 1\n",
            "modelo: 2\n",
            "programação: 2\n",
            "MapReduce: 1\n",
            "popularizado: 1\n",
            "Hadoop,: 2\n",
            "bastante: 1\n",
            "desenvolvimento: 1\n",
            "aplicações: 1\n",
            "volumes: 1\n",
            "dados.: 1\n",
            "Além: 1\n",
            "do: 1\n",
            "performance: 2\n",
            "muito: 1\n",
            "chegando: 1\n",
            "em: 1\n",
            "alguns: 1\n",
            "casos: 1\n",
            "maior.: 1\n",
            "é: 1\n",
            "ferramenta: 1\n",
            "Big: 1\n",
            "Data: 1\n",
            "o: 4\n",
            "de: 9\n",
            "grandes: 2\n",
            "conjuntos: 1\n",
            "forma: 1\n",
            "e: 1\n",
            "distribuída.: 1\n",
            "Ela: 1\n",
            "pelo: 1\n",
            "facilitando: 1\n",
            "processamento: 1\n",
            "estendido,: 1\n",
            "também: 1\n",
            "apresenta: 1\n",
            "superior: 1\n",
            "ao: 1\n",
            "a: 1\n",
            "apresentar: 1\n",
            "quase: 1\n",
            "100x: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_contexto.stop()"
      ],
      "metadata": {
        "id": "OQVxOZSLbZ5K"
      },
      "execution_count": 114,
      "outputs": []
    }
  ]
}